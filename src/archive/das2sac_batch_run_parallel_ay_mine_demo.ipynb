{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "from scipy import signal\n",
    "from obspy import Stream, Trace\n",
    "import gc\n",
    "\n",
    "from nptdms import TdmsFile\n",
    "from DasTools import DasPrep as dp\n",
    "\n",
    "import pyproj\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "from das2sac_batch_run_parallel_func import das_processing, get_das_file_time, das_st_write_sac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt : 0.0025\n",
      "nt : 12000\n",
      "dx : 4.083807945251465\n",
      "nch : 5632\n",
      "GL : 10.0\n",
      "headers : {'AcquisitionDescription': np.bytes_(b''), 'AcquisitionId': np.bytes_(b'12c89c4b-86dc-2709-72f9-e4fe320162b9'), 'FacilityId': array([b'TBD'], dtype='|S3'), 'GaugeLength': np.float64(10.0), 'GaugeLength.uom': np.bytes_(b'm'), 'MaximumFrequency': np.float64(200.0), 'MaximumFrequency.uom': np.bytes_(b'Hz'), 'MeasurementStartTime': np.bytes_(b'2024-07-11T05:51:11.915930+00:00'), 'MinimumFrequency': np.float64(0.0), 'MinimumFrequency.uom': np.bytes_(b'Hz'), 'NumberOfLoci': np.int64(5632), 'PulseRate': np.float64(2000.0), 'PulseRate.uom': np.bytes_(b'Hz'), 'PulseWidth': np.float64(50.0), 'PulseWidth.uom': np.bytes_(b'ns'), 'ServiceCompanyName': np.bytes_(b'Silixa'), 'SpatialSamplingInterval': np.float64(4.083807945251465), 'SpatialSamplingInterval.uom': np.bytes_(b'm'), 'StartLocusIndex': np.int64(0), 'TriggeredMeasurement': np.False_, 'schemaVersion': np.bytes_(b'2.1'), 'uuid': np.bytes_(b'2907cdd1-371e-4143-860c-ca60dc0db993')}\n",
      "0.0025 12000 30.0\n"
     ]
    }
   ],
   "source": [
    "datapath =  '/home/disk/disk01/wzm/DAS_DL_Dataset/data/taiwan/Micro_Seis/EQ_02/'\n",
    "sacpath = '/home/disk/disk01/wzm/DAS_DL_Dataset/data/taiwan/Micro_SAC/EQ_02/'\n",
    "datafile = glob.glob(datapath+'*.h5')\n",
    "datafile.sort()\n",
    "fname_format = datapath + 'XFJ_23km_GL_10m_frq_400Hz_sp_4m_UTC_%Y%m%d_%H%M%S.%f.h5'\n",
    "fname_npdatetime = np.array([np.datetime64(datetime.datetime.strptime(x, fname_format),'us') for x in datafile])\n",
    "fname_datetime = np.array([datetime.datetime.strptime(x, fname_format) for x in datafile])\n",
    "\n",
    "if not os.path.exists(sacpath):  os.makedirs(sacpath)\n",
    "\n",
    "    \n",
    "metadata = dp.read_das(datafile[len(datafile)//2], metadata=True)\n",
    "\n",
    "for key in metadata.keys():\n",
    "    print(key, ':', metadata[key])\n",
    "    \n",
    "\n",
    "dt = metadata['dt']\n",
    "nt = metadata['nt']\n",
    "file_len = dt*nt\n",
    "\n",
    "ch1 = 0\n",
    "ch2 = 5632\n",
    "das_ch_id = np.arange(ch1, ch2)\n",
    "\n",
    "print(dt,nt, file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def get_continuous_segments(fname_datetime, file_len, tol):\n",
    "    fname_datetime_diff = np.diff(fname_datetime)\n",
    "    file_len_timedelta = datetime.timedelta(seconds=file_len*1.0001)\n",
    "    segment_diff = np.where(fname_datetime_diff > file_len_timedelta)[0]  # define continuous segments by no files seperated more than the file length (15s)\n",
    "    segment_start = np.r_[0, segment_diff + 1]\n",
    "    segment_end = np.r_[segment_diff, len(fname_datetime) - 1]\n",
    "\n",
    "    segment_start_datetime = fname_datetime[segment_start] + file_len_timedelta * 1.5 # shift to later by 1.5 file length to give buffer for rolling \n",
    "    segment_end_datetime = fname_datetime[segment_end] - file_len_timedelta * 1.5  # shift to earlier by 1.5 file length to give buffer for rolling \n",
    "\n",
    "    continuous_segment_size = np.array([x.total_seconds() for x in (segment_end_datetime - segment_start_datetime)])\n",
    "    \n",
    "    segment_choose = np.where(continuous_segment_size > tol)[0] \n",
    "    return segment_start_datetime[segment_choose], segment_end_datetime[segment_choose], continuous_segment_size[segment_choose]\n",
    "\n",
    "\n",
    "segment_start_datetime, segment_end_datetime, continuous_segment_size = get_continuous_segments(fname_datetime, file_len, tol=10*60) # segments lasting more than 20 min\n",
    "print(continuous_segment_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([(segment_start_datetime[i], segment_end_datetime[i])\n",
    "  for i in range(len(segment_start_datetime))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for isegment in range(len(segment_start_datetime)):\n",
    "    start_time, end_time = segment_start_datetime[isegment], segment_end_datetime[isegment]\n",
    "    start_time = np.datetime64(start_time)\n",
    "    end_time = np.datetime64(end_time)\n",
    "    segment_size = end_time - start_time\n",
    "    \n",
    "    tmp = segment_size.astype('timedelta64[h]')\n",
    "    print(f'Segment {isegment} : {tmp}')\n",
    "    print(start_time)\n",
    "    print(end_time)\n",
    "    print(' ' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 65 μs, sys: 0 ns, total: 65 μs\n",
      "Wall time: 76.3 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nw = 'XFJ'\n",
    "sta = 'DAS'\n",
    "mlist = np.array([2,5])\n",
    "nprocs = 2\n",
    "\n",
    "chunk_size = np.timedelta64(7200, 's') #increment in hours\n",
    "increment = np.timedelta64(30, 's') # one-time increment of data one thread holds\n",
    "\n",
    "\n",
    "interval = increment * nprocs # total increment of data all threads hold\n",
    "for isegment in range(len(segment_start_datetime)):\n",
    "    \n",
    "    start_time, end_time = segment_start_datetime[isegment], segment_end_datetime[isegment]\n",
    "    start_time = np.datetime64(start_time)\n",
    "    end_time = np.datetime64(end_time)\n",
    "    segment_size = end_time - start_time\n",
    "    chunk_num = segment_size // chunk_size + 1\n",
    "    \n",
    "    print('Segment id: %d'%(isegment))\n",
    "    print('Segment size: %s'%(segment_size.astype('timedelta64[s]'))) \n",
    "    print('Regular chunk size: %s'%(chunk_size))\n",
    "    print('Regular chunk number: %s'%(chunk_num - 1)) \n",
    "    print('Remainder chunk size: %s'%((segment_size % chunk_size).astype('timedelta64[s]')))\n",
    "    \n",
    "    print('nCPU number: %s'%(nprocs)) \n",
    "    print('Increment by each worker: %s'%(increment)) \n",
    "    print('Interval size: %s'%(interval)) \n",
    "    print('Interval number of regular chunks (if any): %s'%(chunk_size // interval))\n",
    "    print('Interval number of the remainder chunk: %s'%(segment_size % chunk_size // interval))\n",
    "    print(' ')\n",
    "    \n",
    "    segment_folder_path = sacpath + 'SAC-segment-' + ''.join(str(start_time.astype('datetime64[s]')).split(':')) \n",
    "    if not os.path.exists(segment_folder_path):\n",
    "        os.makedirs(segment_folder_path)\n",
    "\n",
    "    for ichunk in range(chunk_num):\n",
    "\n",
    "        chunk_start_time = start_time + chunk_size * ichunk\n",
    "\n",
    "        chunk_folder_path = os.path.join(segment_folder_path, \n",
    "                                         'SAC-chunk-' + ''.join(str(chunk_start_time.astype('datetime64[s]')).split(':')))\n",
    "\n",
    "        if os.path.exists(chunk_folder_path):\n",
    "            print('Chunk %d of Segment %d folder already exists: %s'%(ichunk, isegment, chunk_folder_path))\n",
    "            print(' ')\n",
    "            continue\n",
    "            \n",
    "        print('Chunk %d in Segment %d: %s'%(ichunk, isegment, chunk_folder_path))\n",
    "        print('Start time of this chunk: %s'%(chunk_start_time.astype('datetime64[s]')))\n",
    "\n",
    "        data = []\n",
    "        interval_num = chunk_size // interval if ichunk < chunk_num-1 else segment_size % chunk_size // interval\n",
    "        print('Total intervals in this Chunk: %d'%(interval_num))\n",
    "        with Pool(processes=nprocs) as pool:\n",
    "            for j in range(int(interval_num)):\n",
    "                print('Working on Interval %d in Chunk %d in Segment %d'%(j, ichunk, isegment))\n",
    "                increment_start_times = [ chunk_start_time + (interval * j) + (increment * i) for i in range(nprocs) ]\n",
    "                res = pool.map(partial(das_processing, \n",
    "                                       interval=increment, \n",
    "                                       datafile=datafile, \n",
    "                                       datafile_time=fname_npdatetime, \n",
    "                                       ch1=ch1, ch2=ch2, mlist=mlist), \n",
    "                               increment_start_times)\n",
    "\n",
    "                data.append(np.concatenate([res[i][0] for i in range(len(res))], axis=1))\n",
    "                dt = res[0][1]\n",
    "                del res\n",
    "\n",
    "        if len(data) > 0:\n",
    "            data = np.concatenate(data, axis=1)\n",
    "\n",
    "            chunk_start_time_1 = chunk_start_time + increment\n",
    "            datafile_arg_choose = np.where((fname_npdatetime>=chunk_start_time)&(fname_npdatetime<chunk_start_time_1))[0]\n",
    "            chunk_start_time_from_file = fname_npdatetime[datafile_arg_choose][0]\n",
    "\n",
    "            das_st = Stream()\n",
    "            for ich in das_ch_id:\n",
    "\n",
    "                data_ich = np.where(das_ch_id==ich)[0][0]\n",
    "\n",
    "                tr = Trace(data=data[data_ich,:], header={'network':nw, \n",
    "                                                          'station': sta, \n",
    "                                                          'location':str(data_ich), \n",
    "                                                          'channel': str(ich),\n",
    "                                                         'starttime':str(chunk_start_time_from_file), \n",
    "                                                          'delta':dt})\n",
    "\n",
    "                das_st.append(tr)\n",
    "\n",
    "            print('Writing to sac...')\n",
    "            \n",
    "            if not os.path.exists(chunk_folder_path):\n",
    "                os.makedirs(chunk_folder_path)\n",
    "            \n",
    "            with Pool(processes=nprocs) as pool:\n",
    "                pool.map(partial(das_st_write_sac, date_folder_path=sacpath, write_coordinates=False), das_st)\n",
    "\n",
    "            del tr\n",
    "            del das_st\n",
    "            del data\n",
    "        gc.collect()\n",
    "        print(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ditinglsm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
